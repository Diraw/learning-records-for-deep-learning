# ResNet

# 一、基础知识
卷积层越来越所面临的问题
- 梯度消失或梯度爆炸（反向传播过程中不断乘以<1或者>1的梯度）- 进行数据标准化处理，权重初始化，BN(batch normalization)标准化处理（加速训练，丢弃dropout）
- 退化问题 degradation problem（层数越深，但正确率越低）- 残差结构（residual模块）

## 1、residual结构
主分支和侧分支（shortcut）输出的特征矩阵shape必须相同，因为要相加并通过RELU激活函数

相加不仅是对应高和宽维度相加，对应channel的值也要相加，所以三个维度都要对齐

1×1的卷积层可用来降维或升维，且能降低参数量（具体看视频）

虚线部分用于不同残差层相连的部分，因为此处特征矩阵的channel会变，所以测分支会有一个1×1的卷积层经行操作，这样才能保证输出的特征矩阵shape相同

## 2、batch normalization
使每一批的特征矩阵满足均值为0，方差为1的分布规律

## 3、迁移学习
快速、数据集较小却训练出理想的效果，**但要注意别人的1预处理方法**

# 二、ResNeXt
Group Convolution进行分组卷积，可以降低参数量，变为原来的1/g，但是与Google的并行不一样

对于层数小于3的block，这种方法提升不大